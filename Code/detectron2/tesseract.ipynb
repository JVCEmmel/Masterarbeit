{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd01a5e9271f9b78287891653c6c62f0a679986c58645d88e48042d71d8cc8331b4",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tesseract\n",
    "\n",
    "Der hier besprochene Code soll ein kleines Beispiel dafür sein, was im Umgang mit Tesseract und den vom neuronalen Netz erkannten Bounding Boxes für die Kategorie Text möglich ist und wo Limitierungen bestehen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from os.path import isfile, isdir\n",
    "\n",
    "import pytesseract, json, os, cv2\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "#### Funktion  \n",
    "Für die Erfassung der Image-Dateien wurde die folgende Funktion geschrieben. Ihr wird der Dateipfad und ein leeres Set übergeben.  \n",
    "Die Funktion iteriert über jedes Element im Hauptordner. Stößt es auf ein Element, das wiederum ein Ordner ist, ruft sie sich selbst auf - sie iteriert solange durch jeden Ordner bis sie das letzte Element des Hauptordners abgearbeitet hat.  \n",
    "Die Funktion gibt eine geordnete Liste zurück."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(base_path, all_image_names):\n",
    "    file_name_list = os.listdir(base_path)\n",
    "    \n",
    "    for element in file_name_list:\n",
    "        if isfile(base_path + element) & (element.lower().endswith(\".jpg\")):\n",
    "            all_image_names.add((base_path + element).replace(DATASET_PATH, ''))\n",
    "        elif isdir(base_path + element):\n",
    "            get_images(base_path + element + \"/\", all_image_names)\n",
    "\n",
    "    all_image_names = list(all_image_names)\n",
    "    all_image_names.sort()\n",
    "    return all_image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"/home/julius/PowerFolders/Masterarbeit/\"\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "DATASET_PATH = \"./1_Datensaetze/data100/\"\n",
    "JSON_PATH = \"./detections/data100/16,04,2021-12,43/\""
   ]
  },
  {
   "source": [
    "Benötigt wird neben den Bildern die '.json' Datei in der die Koordinaten der Bounding Boxes gespeichert worden sind"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = set()\n",
    "images = get_images(DATASET_PATH, images)\n",
    "\n",
    "with open(JSON_PATH + \"bounding_boxes.json\", \"r+\") as inputfile:\n",
    "    bounding_boxes = json.load(inputfile)"
   ]
  },
  {
   "source": [
    "In einem ersten Schritt werden die Bounding Boxes ermittelt, die zum Label \"Text\" gehören. Sie werden  für jedes Bild in eine Liste gespeichert. Sollte kein Textfeld erkannt worden sein, wird eine leere Liste angelegt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_per_image = {}\n",
    "for count in range(len(images)):\n",
    "    current_image = bounding_boxes[images[count]]\n",
    "\n",
    "    for count_two in range(len(current_image[\"category_names\"])):\n",
    "        if current_image[\"category_names\"][count_two] == \"text\":\n",
    "            if images[count] not in boxes_per_image:\n",
    "                boxes_per_image[images[count]] = [current_image[\"prediction_boxes\"][count_two]]\n",
    "            else:\n",
    "                boxes_per_image[images[count]].append(current_image[\"prediction_boxes\"][count_two])\n",
    "\n",
    "    # if theres no text box in image, create an empty list\n",
    "    if images[count] not in boxes_per_image:\n",
    "        boxes_per_image[images[count]] = []"
   ]
  },
  {
   "source": [
    "#### Funktion\n",
    "\n",
    "In der Funktion _pic_split_ werden die Farbkanäle eines gegebenen Bildes aufgeteilt und zusätzlich binarisiert. Dies wird gemacht um einen weiteren Informationsgewinn aus den Bildern zu ziehen. Neben den vollfarbigen Bildern und den aus diesen generierten graustufenbildern kommen drei weitere Varianten hinzu, die das Abgebildete in anderen Kontrasten zeigen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pic_split(img):\n",
    "\n",
    "    #converting into numpy\n",
    "    img_array = np.array(img).astype(float)\n",
    "\n",
    "    #the actual split\n",
    "    img_array_blue = img_array[:,:,0]\n",
    "    img_array_green = img_array[:,:,1]\n",
    "    img_array_red = img_array[:,:,2]\n",
    "\n",
    "    np.where(img_array_blue<128, 1, 0)\n",
    "    np.where(img_array_green<128, 1, 0)\n",
    "    np.where(img_array_red<128,1 ,0)\n",
    "\n",
    "    #converting them back\n",
    "    img_blue = Image.fromarray(img_array_blue.astype(np.uint8))\n",
    "    img_green = Image.fromarray(img_array_green.astype(np.uint8))\n",
    "    img_red = Image.fromarray(img_array_red.astype(np.uint8))\n",
    "\n",
    "    return img_blue, img_green, img_red"
   ]
  },
  {
   "source": [
    "Die folgende for-Schleife bildet die hauptsächliche Analyse der Bounding Boxes. Hierfür werden zunächst aus den Bildern die Bereiche ausgeschnitten, die in den Boxen liegen. Der Bildinhalt jeder Box wird anschließend transformiert. Neben dem vollfarbigen Bild wird ein Graustufenbild generiert. Um auch die Werte aus den einzelnen Farbschichten zu analysieren, werden mit der _pic_split_ Funktion drei weitere Bilder erzeugt. Die fünf Bilder zeigen jeweils den gleichen Bildinhalt, jedoch unterscheiden sich ihre jeweiligen Pixelwerte.  \n",
    "Dies wird für die Anwendung von OCR wichtig, da bessere Ergebnisse meist mit einem starken Kontrast zwischen den Buchstaben und dem Hintergrund zusammenhängen. In jeder dieser Versionen ist der Kontrast zwischen den einzelnen Pixeln unterschiedlich groß, was zu unterschiedlichen Ergebnissen bei gleichem Bildinhalt führt.  \n",
    "Was sich zeitlich leider nicht mehr realisieren ließ, ist eine weitere Analyse der einzelnen Pixelmatrizen. Es ist denkbar, dass ein Mittelwert der Matrizen bereits Rückschlüsse gibt, ob sich eine bestimmte Variante besser oder weniger gut für OCR eignet. So könnte im Vorfeld am Mittelwert ermittelt werden, welche Varianten am ehesten brauchbare Ergebnisse liefern und nur diese weiter verwenden. Denn mit jeder Bildvariante wird auch der erzeugte Output größer. Mit diesem muss wiederum umgegangen werden, was mehrere Fallstricke beinhaltet.  \n",
    "In diesem Fall wurde die Zeilenstruktur der erkannten Strings beibehalten und nur nach einem von tesseract erkannten Zeilenumbruch getrennt. Dies führt dazu, dass mehrere Worte in einen String fallen, wodurch der Kontext erhalten bleibt. Eine andere Möglichkeit wäre die Trennung nach Leerzeichen gewesen. Durch eine teilweise recht willkürlich wirkende Setzung von Leerzeichen, kann es jedoch sein, dass diese angesprochenen Kontexte verloren gehen. So erkennt tesseract in manchen Datumsangaben Leerzeichen zwischen den Zahlen und Worten. Auch wenn diese nicht immer deplaziert sein müssen würde eine Datumsangabe wie \"13. November 1984\" durch eine Leerzeichentrennung auseinanderfallen. Tauchen mehrer Daten auf, können diese einzelnen Elemente dann vermischt werden und der Sinn verloren gehen. Das gleiche gilt für Wortpaare.  \n",
    "Eine weitere Schwierigkeit ist der Umgang mit Dopplungen. Durch die Anwendung von OCR auf fünf Varianten des gleichen Inhalts können auch fünf Strings mit gleichem oder ähnlichem Inhalt entstehen. Reine Dubletten können über einen Vergleich der von Leerzeichen bereinigten Strings aussortiert werden. Strings, die sich nur in einzelnen Buchstaben von einander unterscheiden, würden beibehalten werden.  \n",
    "Diese müssten zusammen mit den anderen Strings ohne bedeutsamen Inhalt in einem weiteren Schritt ebenfalls gefiltert werden. Denkbare Szenarien wären der Abgleich mit Wörterbüchern und der Ausschluss von einzelnen Worten mit einer Wortlänge von unter eins oder zwei Buchstaben.  \n",
    "Was ebenfalls erheblich zur Verbesserung der Ergebnisse führen kann ist eine Erkennung der Sprache bereits auf der Ebene der Objekterkennung. So wäre es möglich tesseract explizit zu sagen, welche Sprache zu erkennen ist, was die Erkennung von Sonderzeichen deutlich verbessern sollte.  \n",
    "Dies alles macht deutlich, dass es Vorteilhaft wäre, neben dem reinen \"Text\" Label spezifischere Label wie \"Datum\" und \"Ortsangabe\" beziehungsweise ein Sprachlabel zu haben. Somit könnten unterschiedliche Verfahrensweisen mit dem zu erkennenden Text etabliert und die gesamte Qualität der Ergebnisse gesteigert werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(len(images)):\n",
    "    # load image\n",
    "    image_dict = {}\n",
    "    image_dict[\"complete_image\"] = cv2.imread(DATASET_PATH + images[element])\n",
    "    dump_dict = {}\n",
    "\n",
    "    ###CONSOLE OUTPUT###\n",
    "    print(\"[INFO] Read Image: {}.\".format(images[element]))\n",
    "\n",
    "    # go over every box in the image\n",
    "    for box in range(len(boxes_per_image[images[element]])):\n",
    "        # get box cords, length and with and cut text out\n",
    "        x_one = boxes_per_image[images[element]][box][0]\n",
    "        x_two = boxes_per_image[images[element]][box][1]\n",
    "        y_one = boxes_per_image[images[element]][box][2]\n",
    "        y_two = boxes_per_image[images[element]][box][3]\n",
    "\n",
    "        box_width = abs(x_one - y_one)\n",
    "        box_height = abs(x_two - y_two)\n",
    "\n",
    "        image_dict[\"colored_box\"] = image_dict[\"complete_image\"][x_two : x_two + box_height, x_one : x_one + box_width]\n",
    "        image_dict[\"blue_box\"], image_dict[\"green_box\"], image_dict[\"red_box\"] = pic_split(image_dict[\"colored_box\"])\n",
    "        image_dict[\"gray_box\"] = cv2.cvtColor(image_dict[\"colored_box\"], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # OCR on every variant\n",
    "        for variant in image_dict:\n",
    "            text_dump = set()\n",
    "\n",
    "            if (variant != \"complete_image\"):\n",
    "                image_text = \"{}\".format(pytesseract.image_to_string(image_dict[variant], lang=\"deu\"))\n",
    "                image_text = image_text.replace(\"\\x0c\", \"\").split(\"\\n\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if len(image_text) > 0:\n",
    "                [text_dump.add(text) for text in image_text if len(text) > 0]\n",
    "            \n",
    "            if variant not in dump_dict:\n",
    "                dump_dict[variant] = list(text_dump)\n",
    "            else:\n",
    "                dump_dict[variant].append(list(text_dump))\n",
    "            \n",
    "        ###CONSOLE OUTPUT###\n",
    "        print(\"[INFO] Finished Detection for box {}/{} of image {}\".format(box+1, len(boxes_per_image[images[element]]), images[element]))\n",
    "\n",
    "    # OCR on the whole image\n",
    "    image_text = \"{}\".format(pytesseract.image_to_string(image_dict[\"complete_image\"], lang=\"deu\"))\n",
    "    image_text = image_text.replace(\"\\x0c\", \"\").split(\"\\n\")\n",
    "    if len(image_text) > 0:\n",
    "        [text_dump.add(text) for text in image_text if len(text) > 0]\n",
    "    dump_dict[\"complete_image\"] = list(text_dump)\n",
    "\n",
    "    # dump results\n",
    "    with open(JSON_PATH + \"{}.json\".format(images[element].split(\"/\")[-1][:-4]), \"w+\", encoding=\"utf8\") as output_file:\n",
    "        json.dump(dump_dict, output_file, indent=4, ensure_ascii=False)"
   ]
  }
 ]
}